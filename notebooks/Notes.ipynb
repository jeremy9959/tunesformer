{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a787dc94-d525-4ee3-90fc-422d8909ab70\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"a787dc94-d525-4ee3-90fc-422d8909ab70\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a787dc94-d525-4ee3-90fc-422d8909ab70\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "output_notebook()\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from utils import *\n",
    "from config import *\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, get_scheduler\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size 1\n",
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = torch.cuda.device_count()\n",
    "print(f\"Using batch size {batch_size}\")\n",
    "patchilizer = Patchilizer()\n",
    "device = \"cpu\"\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_config = GPT2Config(num_hidden_layers=PATCH_NUM_LAYERS, \n",
    "                    max_length=PATCH_LENGTH, \n",
    "                    max_position_embeddings=PATCH_LENGTH,\n",
    "                    vocab_size=1)\n",
    "char_config = GPT2Config(num_hidden_layers=CHAR_NUM_LAYERS, \n",
    "                    max_length=PATCH_SIZE, \n",
    "                    max_position_embeddings=PATCH_SIZE,\n",
    "                    vocab_size=128)\n",
    "model = TunesFormer(patch_config, char_config, share_weights=SHARE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The char level decoder has 3 GPT2 blocks and a \"linear\" head that converts the 768 dimensional embeddings\n",
    "into 128 logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLevelDecoder(\n",
      "  (base): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(128, 768)\n",
      "      (wpe): Embedding(32, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-2): 3 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=128, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.char_level_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"patch level decoder\" has 9 GPT2 blocks. Its input is 4096 dimensional,\n",
    "corresponding to \"patches\" or \"measures\" that have (up to) 32 symbols, each chosen from a 128 character vocabulary.\n",
    "These get embedded in 768 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchLevelDecoder(\n",
      "  (patch_embedding): Linear(in_features=4096, out_features=768, bias=True)\n",
      "  (base): GPT2Model(\n",
      "    (wte): Embedding(1, 768)\n",
      "    (wpe): Embedding(128, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-8): 9 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.patch_level_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of the Tunesformer Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, one can get lots of information about abc at the [abc notation home page](https://abcnotation.com/).  Let's get a copy of whiskey before breakfast to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abcs/whiskey-before-breakfast.abc\") as f:\n",
    "    whiskey = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from an amalgamation of abc files from a variety of sources and is stored on huggingface in a dataset called\n",
    "`irishman`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023.10.0\n",
      "2.16.1\n",
      "4.32.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import fsspec\n",
    "import transformers\n",
    "print(fsspec.__version__)\n",
    "print(datasets.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consists of 214122 training rows and 2162 validation rows\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "irishman = load_dataset(\"sander-wood/irishman\")\n",
    "print(f\"Consists of {irishman['train'].shape[0]} training rows and {irishman['validation'].shape[0]} validation rows\")\n",
    "irishman_df = pd.DataFrame(irishman['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = r\"(K:(?P<key>[\\w]+))\"\n",
    "time_sig = r\"(M:(?P<time_sig>[\\d]+\\/[\\d]+))\"\n",
    "def extract_key(abc):\n",
    "   m = re.search(key, abc)\n",
    "   return m.group('key') if m else None\n",
    "\n",
    "def extract_tonic(abc):\n",
    "    m = extract_key(abc)\n",
    "    return None if m=='none' or not m else m[0]\n",
    "\n",
    "\n",
    "def extract_mode(abc):\n",
    "    \n",
    "    m = extract_key(abc)\n",
    "    if not m or m=='none':\n",
    "        return None\n",
    "    mode = m[1:]\n",
    "    if len(mode)==0 or mode=='maj':\n",
    "        return 'major'\n",
    "    if mode=='min':\n",
    "        return 'minor'\n",
    "    return 'modal'\n",
    "    \n",
    "\n",
    "def extract_time_sig(abc):\n",
    "    m=re.search(time_sig, abc)\n",
    "    return m.group('time_sig') if m else None\n",
    "\n",
    "irishman_df['key']=irishman_df['abc notation'].apply(extract_key)\n",
    "irishman_df['time_sig']=irishman_df['abc notation'].apply(extract_time_sig)\n",
    "irishman_df['tonic']=irishman_df['abc notation'].apply(extract_tonic)\n",
    "irishman_df['mode']=irishman_df['abc notation'].apply(extract_mode)\n",
    "\n",
    "irishman_df['key_mode'] = irishman_df['key'] + '_' + irishman_df['mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2500 songs without a key indication.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_keys = (irishman_df['key']=='none')\n",
    "missing_keys.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pd.DataFrame(irishman_df.groupby(['tonic','mode']).size().reset_index(name='count'))\n",
    "keys['key_mode'] = keys['tonic'] + keys['mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"ae9cfb07-7ba4-4b6d-bd94-1a2c700b484f\" data-root-id=\"p1001\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"5140de07-8c51-412b-af63-f5794ba426b6\":{\"version\":\"3.3.4\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1001\",\"attributes\":{\"height\":500,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1003\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1011\",\"attributes\":{\"factors\":[\"Fminor\",\"Fmodal\",\"Bmajor\",\"Cmodal\",\"Cminor\",\"Emajor\",\"Gmodal\",\"Bminor\",\"Gminor\",\"Dminor\",\"Dmodal\",\"Emodal\",\"Aminor\",\"Bmodal\",\"Eminor\",\"Amodal\",\"Fmajor\",\"Amajor\",\"Cmajor\",\"Dmajor\",\"Gmajor\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1012\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1013\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1004\",\"attributes\":{\"text\":\"Key Counts \"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1030\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1024\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1025\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1026\"},\"data\":{\"type\":\"map\",\"entries\":[[\"y\",{\"type\":\"ndarray\",\"array\":[\"Fminor\",\"Fmodal\",\"Bmajor\",\"Cmodal\",\"Cminor\",\"Emajor\",\"Gmodal\",\"Bminor\",\"Gminor\",\"Dminor\",\"Dmodal\",\"Emodal\",\"Aminor\",\"Bmodal\",\"Eminor\",\"Amodal\",\"Fmajor\",\"Amajor\",\"Cmajor\",\"Dmajor\",\"Gmajor\"],\"shape\":[21],\"dtype\":\"object\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"fAAAAMsAAADPAAAAKQEAAOwBAACbBQAAKAcAAKkKAACiDgAAqA8AAGYQAAD+FQAADBgAAOUYAAArGgAA1CAAAF0xAAD+PAAAE1QAADPBAADI6QAA\"},\"shape\":[21],\"dtype\":\"int32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1031\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1032\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1027\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"y\"},\"height\":{\"type\":\"value\",\"value\":0.9},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"line_color\":{\"type\":\"value\",\"value\":\"pink\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1028\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"y\"},\"height\":{\"type\":\"value\",\"value\":0.9},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"line_color\":{\"type\":\"value\",\"value\":\"pink\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1029\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"y\"},\"height\":{\"type\":\"value\",\"value\":0.9},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"line_color\":{\"type\":\"value\",\"value\":\"pink\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1010\"},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1019\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1020\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1021\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1022\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1014\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1015\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1016\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1017\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1018\",\"attributes\":{\"axis\":{\"id\":\"p1014\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1023\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1019\"}}}]}}]}};\n  const render_items = [{\"docid\":\"5140de07-8c51-412b-af63-f5794ba426b6\",\"roots\":{\"p1001\":\"ae9cfb07-7ba4-4b6d-bd94-1a2c700b484f\"},\"root_ids\":[\"p1001\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1001"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "keys = keys[keys['key_mode']!='none']\n",
    "keys = keys.sort_values(by='count', ascending=True)\n",
    "p = figure(y_range=keys['key_mode'], height=500, title=f\"Key Counts \",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "\n",
    "p.hbar(y=keys['key_mode'], right=keys['count'], height=0.9,fill_color='gray',line_color='pink')\n",
    "\n",
    "p.x_range.start = 0\n",
    "#p.xaxis.major_label_orientation = \"vertical\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some unusual time signatures (which are clearly errors) in the dataset. For example, 9/81,\n",
    "or 10/16, or 432/444 (!) seem likely to be mistakes.   For now we won't worry about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2,1/4,1/8,10/16,10/4,10/8,11/16,11/8,12/16,12/6,13/16,13/28,13/4,13/8,14/8,15/16,15/8,17/8,18/16,18/4\n",
      "18/8,2/1,2/3,21/4,22/16,22/8,23/4,26/8,28/4,3/16,3/3,3/6,32/44,4/3,43/44,432/444,45/44,46/8,5/16,5/2\n",
      "6/16,6/5,6/6,6/86,6/9,63/84,7/4,8/16,8/2,8/4,9/12,9/3,9/6,9/81,"
     ]
    }
   ],
   "source": [
    "sigs = pd.DataFrame(irishman_df.groupby('time_sig').size().reset_index(name='count'))\n",
    "for i,x in enumerate(sigs[sigs['count']<20]['time_sig']):\n",
    "    if i % 20 < 19:\n",
    "        print(f\"{x},\",end=\"\")\n",
    "    else:\n",
    "        print(f\"{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library has a class called \"Patchilizer\" which functions as the tokenizer.  I think it's called \"patchilizer\" because it operates on the bar level.  The idea here is that you can think of the data as having a hierarchical structure made up of notes within bars. \n",
    "\n",
    "At its root the tokenization is on the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar 0: \n",
      "\n",
      "X:1\n",
      "T:Whiskey Before Breakfast\n",
      "L:1/8\n",
      "M:4/4\n",
      "Z:abc-transcription Josh Larios <hades@elsewhere.org>, 2014.01.13\n",
      "B:Complete Tractor, p.210\n",
      "N:The bluegrassers all play that E minor chord in measure 11, but most old time backup players just play A.\n",
      "K:D\n",
      "|:\n",
      "Bar 1:  \"D\"DE FG A2 AA |\n",
      "Bar 2:  AB AG FE DF |\n",
      "Bar 3:  \"G\"G2 BG \"D\"F2 AF |\n",
      "Bar 4:  \"A\"ED EF EC B,A, |\n",
      "Bar 5: \n",
      "\"D\"DE FG A2 AA |\n",
      "Bar 6:  AB AG FE DF |\n",
      "Bar 7:  \"G\"G2 BG \"D\"F2 AF |\n",
      "Bar 8:  \"A\"ED EF \"D\"D2 A2 ::\n",
      "Bar 9: \n",
      "\"D\"A2 d2 d2 dd |\n",
      "Bar 10:  f2 d2 B2 A2 |\n",
      "Bar 11:  \"Em (A)\"e2 ef e2 ef |\n",
      "Bar 12:  \"A7\" gf ed cB Ac |\n",
      "Bar 13: \n",
      "\"D\"d2 fd \"A\"c2 ec |\n",
      "Bar 14:  \"G\"Bc dB \"D\"AF ED |\n",
      "Bar 15:  \"G\"G2 BG \"D\"F2 AF |\n",
      "Bar 16:  \"A\"ED EF \"D\"D2-D2 :|\n"
     ]
    }
   ],
   "source": [
    "from utils import Patchilizer\n",
    "P = Patchilizer()\n",
    "encoded = P.encode(whiskey)\n",
    "for i,x in enumerate(P.split_bars(whiskey)):\n",
    "    print(f\"Bar {i}: {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding into \"patches\" puts the control lines into individual patches, and then collects each measure into a patch, where the initial bar (if any) goes into the first patch and patches end with a '|' or related delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch 0: X-:-1-\n",
      "\n",
      "Patch 1: T-:-W-h-i-s-k-e-y- -B-e-f-o-r-e- -B-r-e-a-k-f-a-s-t-\n",
      "\n",
      "Patch 2: L-:-1-/-8-\n",
      "\n",
      "Patch 3: M-:-4-/-4-\n",
      "\n",
      "Patch 4: Z-:-a-b-c---t-r-a-n-s-c-r-i-p-t-i-o-n- -J-o-s-h- -L-a-r-i-o-s\n",
      "Patch 5: B-:-C-o-m-p-l-e-t-e- -T-r-a-c-t-o-r-,- -p-.-2-1-0-\n",
      "\n",
      "Patch 6: N-:-T-h-e- -b-l-u-e-g-r-a-s-s-e-r-s- -a-l-l- -p-l-a-y- -t-h-a\n",
      "Patch 7: K-:-D-\n",
      "\n",
      "Patch 8: |-:- -\"-D-\"-D-E- -F-G- -A-2- -A-A- -|\n",
      "Patch 9:  -A-B- -A-G- -F-E- -D-F- -|\n",
      "Patch 10:  -\"-G-\"-G-2- -B-G- -\"-D-\"-F-2- -A-F- -|\n",
      "Patch 11:  -\"-A-\"-E-D- -E-F- -E-C- -B-,-A-,- -|\n",
      "Patch 12: \n",
      "-\"-D-\"-D-E- -F-G- -A-2- -A-A- -|\n",
      "Patch 13:  -A-B- -A-G- -F-E- -D-F- -|\n",
      "Patch 14:  -\"-G-\"-G-2- -B-G- -\"-D-\"-F-2- -A-F- -|\n",
      "Patch 15:  -\"-A-\"-E-D- -E-F- -\"-D-\"-D-2- -A-2- -:-:\n",
      "Patch 16: \n",
      "-\"-D-\"-A-2- -d-2- -d-2- -d-d- -|\n",
      "Patch 17:  -f-2- -d-2- -B-2- -A-2- -|\n",
      "Patch 18:  -\"-E-m- -(-A-)-\"-e-2- -e-f- -e-2- -e-f- -|\n",
      "Patch 19:  -\"-A-7-\"- -g-f- -e-d- -c-B- -A-c- -|\n",
      "Patch 20: \n",
      "-\"-D-\"-d-2- -f-d- -\"-A-\"-c-2- -e-c- -|\n",
      "Patch 21:  -\"-G-\"-B-c- -d-B- -\"-D-\"-A-F- -E-D- -|\n",
      "Patch 22:  -\"-G-\"-G-2- -B-G- -\"-D-\"-F-2- -A-F- -|\n",
      "Patch 23:  -\"-A-\"-E-D- -E-F- -\"-D-\"-D-2---D-2- -:-|\n"
     ]
    }
   ],
   "source": [
    "for i,r in enumerate(encoded):\n",
    "    print(f\"Patch {i}: {'-'.join([chr(x) for x in r if x>2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data comes from the `irishman['train']` dataset.  This is a generator that yields dictionaries.\n",
    "The dictionaries have two keys: `abc notation` and `control code`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['abc notation', 'control code'],\n",
       "    num_rows: 214122\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irishman['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control code:\n",
      "S:2\n",
      "B:5\n",
      "E:5\n",
      "B:6\n",
      "\n",
      "ABC:\n",
      "X:1\n",
      "L:1/8\n",
      "M:4/4\n",
      "K:Emin\n",
      "|: E2 EF E2 EF | DEFG AFDF | E2 EF E2 B2 |1 efe^d e2 e2 :|2 efe^d e3 B |: e2 ef g2 fe | \n",
      " defg afdf |1 e2 ef g2 fe | efe^d e3 B :|2 g2 bg f2 af | efe^d e2 e2 ||\n"
     ]
    }
   ],
   "source": [
    "print(f\"Control code:\\n{irishman['train'][0]['control code']}\\nABC:\\n{irishman['train'][0]['abc notation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data passed to the training loop is encoded using the \"patchilizer\".  It appears we drop the X: key at the start of each\n",
    "abc tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:2\n",
      "B:5\n",
      "E:5\n",
      "B:6\n",
      "L:1/8\n",
      "M:4/4\n",
      "K:Emin\n",
      "|: E2 EF E2 EF | DEFG AFDF | E2 EF E2 B2 |1 efe^d e2 e2 :|2 efe^d e3 B |: e2 ef g2 fe | \n",
      " defg afdf |1 e2 ef g2 fe | efe^d e3 B :|2 g2 bg f2 af | efe^d e2 e2 ||\n"
     ]
    }
   ],
   "source": [
    "P = Patchilizer()\n",
    "item = irishman['train'][0]\n",
    "text = item['control code']+\"\\n\".join(item['abc notation'].split('\\n')[1:])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input patch is a m x PATCHSIZE torch tensor where m is the number of bars. In our case\n",
    "PATCHSIZE=32. A special initial and ending bar is attached (the initial one is [bos, bos, ... bos,eos]\n",
    "and the final one is [bos, eos, eos,....eos])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 32])\n"
     ]
    }
   ],
   "source": [
    "input_patch = torch.tensor(P.encode(text,add_special_patches=True))\n",
    "print(input_patch.unsqueeze(0).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size is set to 1 (assuming only one GPU, in fact batch size is number of GPU's available.) Although the code fiddles\n",
    "around quite a bit with sizes, what actually happens is that the tune with M bars is encoded into an M x PATCH_SIZE = M x 32\n",
    "tensor, this gets \"unsqueezed\" into a 1 x M x PATCH_SIZE array, and that gets supplied to the model.\n",
    "\n",
    "The first level of the model, the `patch_level_decoder`, returns a 1 x M x 768 (1 x M x embedding_dimension) vector for the patches.\n",
    "\n",
    "This is (part of) the input to the `char_level_decoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "embedding = model.patch_level_decoder(input_patch.unsqueeze(0))[\"last_hidden_state\"]\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
